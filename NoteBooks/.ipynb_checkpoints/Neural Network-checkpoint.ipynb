{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Network\n",
    "=============="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing required python modules\n",
    "---------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import optimize\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following libraries have been used :\n",
    "- ** Numpy **: NumPy is the fundamental package for scientific computing with Python.\n",
    "- ** Scipy **: Scipy is a collection of numerical algorithms and domain-specific toolboxes, including signal processing, optimization, statistics and much more.\n",
    "- ** Sklearn **: It features various classification, regression and clustering algorithms including support vector machines, random forests, gradient boosting, k-means and DBSCAN, and is designed to interoperate with the Python numerical and scientific libraries NumPy and SciPy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning from the data\n",
    "-------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def featureNormalize(z):\n",
    "    return scale(z)\n",
    "def sigmoid(z):\n",
    "    r = 1.0 / (1.0 + np.exp(-z))\n",
    "    return r\n",
    "def sigmoidGrad(z):\n",
    "    r = sigmoid(z)\n",
    "    r = r * (1.0 - r)\n",
    "    return r\n",
    "def randomizeTheta(l, epsilon):\n",
    "    return ((np.random.random((l, 1)) * 2 * epsilon) - epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def KFoldDiv(X, y, m, n, K):\n",
    "    sz = int(np.ceil(m / K))\n",
    "    if n == 1:\n",
    "        X_train = X[sz:, :]\n",
    "        X_test = X[:sz, :]\n",
    "        y_train = y[sz:]\n",
    "        y_test = y[:sz]\n",
    "    elif n == K:\n",
    "        X_train = X[:((n-1)*sz), :]\n",
    "        X_test = X[((n-1)*sz):, :]\n",
    "        y_train = y[:((n-1)*sz)]\n",
    "        y_test = y[((n-1)*sz):]\n",
    "    else:\n",
    "        X_train = np.vstack((X[:((n-1)*sz), :], X[(n*sz):, :]))\n",
    "        X_test = X[((n-1)*sz):(n*sz), :]\n",
    "        y_train = np.vstack((y[:((n-1)*sz)], y[(n*sz):]))\n",
    "        y_test = y[((n-1)*sz):(n*sz)]\n",
    "    return (X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Auxiliary Functions***:\n",
    "- ** featureNormalize **: Scales the attributes of the dataset.\n",
    "- ** sigmoid **: Computes sigmoid function on the given data.\n",
    "- ** sigmoidGrad **: Computes derivative of sigmoid function on the given data.\n",
    "- ** randomizeTheta **: Generates a set of random weights for the purpose of initialization of weights.\n",
    "- ** KFoldDiv **: It is a function which divides the dataset into train and test datasets, based on the fold number for cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nnCostFunc(Theta, input_layer_size, hidden_layer_size, num_labels, X, y, lmbda):\n",
    "    Theta1, Theta2 = np.split(Theta, [hidden_layer_size * (input_layer_size+1)])\n",
    "    Theta1 = np.reshape(Theta1, (hidden_layer_size, input_layer_size+1))\n",
    "    Theta2 = np.reshape(Theta2, (num_labels, hidden_layer_size+1))\n",
    "    m = X.shape[0]\n",
    "    y = (y == np.array([(i+1) for i in range(num_labels)])).astype(int)\n",
    "\n",
    "    a1 = np.hstack((np.ones((m, 1)), X))\n",
    "    z2 = np.dot(a1, Theta1.T)\n",
    "    a2 = np.hstack((np.ones((m, 1)), sigmoid(z2)))\n",
    "    h = sigmoid(np.dot(a2, Theta2.T))\n",
    "\n",
    "    cost = ((lmbda/2)*(np.sum(Theta1[:, 1:] ** 2) +\n",
    "            np.sum(Theta2[:, 1:] ** 2)) -\n",
    "            np.sum((y * np.log(h)) +\n",
    "            ((1-y) * np.log(1-h)))) / m\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**nnCostFunc**: It computes the cost function for neural networks with regularization, which is given by,\n",
    "\n",
    "$$\n",
    "Cost(θ) = \\frac{1}{m}\\sum_{i=1}^m\\sum_{k=1}^K\\left[ -y_k^{(i)}\\ln{((h_θ(x^{(i)}))_k)} - (1 - y_k^{(i)})\\ln{(1 - (h_θ(x^{(i)}))_k)}\\right] + \\frac{\\lambda}{2m}\\left[\\sum_{i=1}(θ_i)^2\\right]\n",
    "$$\n",
    "\n",
    "The neural network has 3 layers – an input layer, a hidden layer and an output layer. It uses forward propagation to compute $(h_θ(x^{(i)}))_k$, the activation (output value) of the k-th output unit and θ represents the weights. The code works for any number of input units, hidden units and outputs units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nnGrad(Theta, input_layer_size, hidden_layer_size, num_labels, X, y, lmbda):\n",
    "    Theta1, Theta2 = np.split(Theta, [hidden_layer_size * (input_layer_size+1)])\n",
    "    Theta1 = np.reshape(Theta1, (hidden_layer_size, input_layer_size+1))\n",
    "    Theta2 = np.reshape(Theta2, (num_labels, hidden_layer_size+1))\n",
    "    m = X.shape[0]\n",
    "    y = (y == np.array([(i+1) for i in range(num_labels)])).astype(int)\n",
    "\n",
    "    a1 = np.hstack((np.ones((m, 1)), X))\n",
    "    z2 = np.dot(a1, Theta1.T)\n",
    "    a2 = np.hstack((np.ones((m, 1)), sigmoid(z2)))\n",
    "    h = sigmoid(np.dot(a2, Theta2.T))\n",
    "\n",
    "    delta_3 = h - y\n",
    "    delta_2 = np.dot(delta_3, Theta2[:, 1:]) * sigmoidGrad(z2)\n",
    "    Theta2_grad = (np.dot(delta_3.T, a2) + \n",
    "                   (lmbda * np.hstack((np.zeros((Theta2.shape[0], 1)),\n",
    "                                       Theta2[:, 1:])))) / m\n",
    "    Theta1_grad = (np.dot(delta_2.T, a1) +\n",
    "                   (lmbda * np.hstack((np.zeros((Theta1.shape[0], 1)),\n",
    "                                       Theta1[:, 1:])))) / m\n",
    "\n",
    "    grad = np.hstack((Theta1_grad.flatten(), Theta2_grad.flatten()))\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**nnGrad**: It computes the gradient(also called partial derivative) of the cost function with respect to all weights in the neural network. The gradient helps in optimizing the weights in order to minimize the value of the cost function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "K = 10\n",
    "lmbda = 0.03\n",
    "epsilon = 0.12\n",
    "\n",
    "input_layer_size = 13\n",
    "hidden_layer_size = 20\n",
    "num_labels = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialisation of relevant parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.genfromtxt('heart.csv', delimiter=',')\n",
    "m, n = X.shape\n",
    "n -= 1\n",
    "\n",
    "y = X[:, n].astype(int).reshape((m, 1))\n",
    "X = featureNormalize(X[:, :n])\n",
    "foldAcc = np.ndarray((K, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the dataset and extract labels and attributes from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FP = 0\n",
    "FN = 0\n",
    "TN = 0\n",
    "TP = 0\n",
    "for i in range(K):\n",
    "    X_train, y_train, X_test, y_test = KFoldDiv(X, y, m, i+1, K)\n",
    "    \n",
    "    initTheta = randomizeTheta((hidden_layer_size * (input_layer_size+1)) +\n",
    "                               (num_labels * (hidden_layer_size+1)), epsilon)\n",
    "    Theta = optimize.fmin_bfgs(nnCostFunc, initTheta, fprime=nnGrad,\n",
    "                               args=(input_layer_size,\n",
    "                                     hidden_layer_size,\n",
    "                                     num_labels, X_train,\n",
    "                                     y_train,\n",
    "                                     lmbda),\n",
    "                               maxiter=3000)\n",
    "    Theta1, Theta2 = np.split(Theta, [hidden_layer_size * (input_layer_size+1)])\n",
    "    Theta1 = np.reshape(Theta1, (hidden_layer_size, input_layer_size+1))\n",
    "    Theta2 = np.reshape(Theta2, (num_labels, hidden_layer_size+1))\n",
    "\n",
    "    h1 = sigmoid(np.dot(np.hstack((np.ones((X_test.shape[0], 1)), X_test)), Theta1.T))\n",
    "    h2 = sigmoid(np.dot(np.hstack((np.ones((h1.shape[0], 1)), h1)), Theta2.T))\n",
    "    predicted = h2.argmax(1) + 1\n",
    "    predicted = predicted.reshape((predicted.shape[0], 1))\n",
    "    foldAcc[i] = np.mean((predicted == y_test).astype(float)) * 100\n",
    "\n",
    "    cm = (metrics.confusion_matrix(y_test, predicted))/len(y_test)\n",
    "\n",
    "    FP += cm[0][0]\n",
    "    FN += cm[1][0]\n",
    "    TN += cm[0][1]\n",
    "    TP += cm[1][1]\n",
    "\n",
    "    print('Test Set Accuracy for %dth fold: %f\\n' % (i+1, foldAcc[i]))\n",
    "meanAcc = np.mean(foldAcc)\n",
    "print('\\nAverage Accuracy: ', meanAcc)\n",
    "print(\"\")\n",
    "print(FP)\n",
    "print(FN)\n",
    "print(TN)\n",
    "print(TP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above written code is used to run 10 Fold Cross Validation on the Neural Network and display the Model Accuracy and the Confusion Matrix and related metrics.\n",
    "\n",
    "**fmin_bfgs** function from **Scipy** library is used to optimize the weights in order to minimize the cost, using the BFGS algorithm.\n",
    "\n",
    "Parameters:\n",
    "- f : callable f(x,\\*args), *Objective function to be minimized.*\n",
    "- x0 : ndarray, *Initial guess.*\n",
    "- fprime : callable f’(x,\\*args), *Gradient of f.*\n",
    "- args : tuple, *Extra arguments passed to f and fprime.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
