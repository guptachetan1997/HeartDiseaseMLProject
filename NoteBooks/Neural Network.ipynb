{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Network\n",
    "=============="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing required python modules\n",
    "---------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import optimize\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following libraries have been used :\n",
    "- ** Numpy **: NumPy is the fundamental package for scientific computing with Python.\n",
    "- ** Scipy **: Scipy is a collection of numerical algorithms and domain-specific toolboxes, including signal processing, optimization, statistics and much more.\n",
    "- ** Sklearn **: It features various classification, regression and clustering algorithms including support vector machines, random forests, gradient boosting, k-means and DBSCAN, and is designed to interoperate with the Python numerical and scientific libraries NumPy and SciPy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning from the data\n",
    "-------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def featureNormalize(z):\n",
    "    return scale(z)\n",
    "\n",
    "def sigmoid(z):\n",
    "    r = 1.0 / (1.0 + np.exp(-z))\n",
    "    return r\n",
    "\n",
    "def sigmoidGrad(z):\n",
    "    r = sigmoid(z)\n",
    "    r = r * (1.0 - r)\n",
    "    return r\n",
    "\n",
    "def randomizeTheta(l, epsilon):\n",
    "    return ((np.random.random((l, 1)) * 2 * epsilon) - epsilon)\n",
    "\n",
    "def KFoldDiv(X, y, m, n, K):\n",
    "    sz = int(np.ceil(m / K))\n",
    "    if n == 1:\n",
    "        X_train = X[sz:, :]\n",
    "        X_test = X[:sz, :]\n",
    "        y_train = y[sz:]\n",
    "        y_test = y[:sz]\n",
    "    elif n == K:\n",
    "        X_train = X[:((n-1)*sz), :]\n",
    "        X_test = X[((n-1)*sz):, :]\n",
    "        y_train = y[:((n-1)*sz)]\n",
    "        y_test = y[((n-1)*sz):]\n",
    "    else:\n",
    "        X_train = np.vstack((X[:((n-1)*sz), :], X[(n*sz):, :]))\n",
    "        X_test = X[((n-1)*sz):(n*sz), :]\n",
    "        y_train = np.vstack((y[:((n-1)*sz)], y[(n*sz):]))\n",
    "        y_test = y[((n-1)*sz):(n*sz)]\n",
    "    return (X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Auxiliary Functions***:\n",
    "- ** featureNormalize **: Scales the attributes of the dataset.\n",
    "- ** sigmoid **: Computes sigmoid function on the given data.\n",
    "- ** sigmoidGrad **: Computes derivative of sigmoid function on the given data.\n",
    "- ** randomizeTheta **: Generates a set of random weights for the purpose of initialization of weights.\n",
    "- ** KFoldDiv **: It is a function which divides the dataset into train and test datasets, based on the fold number for cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nnCostFunc(Theta, input_layer_size, hidden_layer_size, num_labels, X, y, lmbda):\n",
    "    Theta1, Theta2 = np.split(Theta, [hidden_layer_size * (input_layer_size+1)])\n",
    "    Theta1 = np.reshape(Theta1, (hidden_layer_size, input_layer_size+1))\n",
    "    Theta2 = np.reshape(Theta2, (num_labels, hidden_layer_size+1))\n",
    "    m = X.shape[0]\n",
    "    y = (y == np.array([(i+1) for i in range(num_labels)])).astype(int)\n",
    "\n",
    "    a1 = np.hstack((np.ones((m, 1)), X))\n",
    "    z2 = np.dot(a1, Theta1.T)\n",
    "    a2 = np.hstack((np.ones((m, 1)), sigmoid(z2)))\n",
    "    h = sigmoid(np.dot(a2, Theta2.T))\n",
    "\n",
    "    cost = ((lmbda/2)*(np.sum(Theta1[:, 1:] ** 2) + np.sum(Theta2[:, 1:] ** 2)) - np.sum((y * np.log(h)) + ((1-y) * np.log(1-h)))) / m\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**nnCostFunc**: It computes the cost function for neural networks with regularization, which is given by,\n",
    "\n",
    "$$\n",
    "Cost(θ) = \\frac{1}{m}\\sum_{i=1}^m\\sum_{k=1}^K\\left[ -y_k^{(i)}\\ln{((h_θ(x^{(i)}))_k)} - (1 - y_k^{(i)})\\ln{(1 - (h_θ(x^{(i)}))_k)}\\right] + \\frac{\\lambda}{2m}\\left[\\sum_{i=1}(θ_i)^2\\right]\n",
    "$$\n",
    "\n",
    "The neural network has 3 layers – an input layer, a hidden layer and an output layer. It uses forward propagation to compute $(h_θ(x^{(i)}))_k$, the activation (output value) of the k-th output unit and θ represents the weights. The code works for any number of input units, hidden units and outputs units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nnGrad(Theta, input_layer_size, hidden_layer_size, num_labels, X, y, lmbda):\n",
    "    Theta1, Theta2 = np.split(Theta, [hidden_layer_size * (input_layer_size+1)])\n",
    "    Theta1 = np.reshape(Theta1, (hidden_layer_size, input_layer_size+1))\n",
    "    Theta2 = np.reshape(Theta2, (num_labels, hidden_layer_size+1))\n",
    "    m = X.shape[0]\n",
    "    y = (y == np.array([(i+1) for i in range(num_labels)])).astype(int)\n",
    "\n",
    "    a1 = np.hstack((np.ones((m, 1)), X))\n",
    "    z2 = np.dot(a1, Theta1.T)\n",
    "    a2 = np.hstack((np.ones((m, 1)), sigmoid(z2)))\n",
    "    h = sigmoid(np.dot(a2, Theta2.T))\n",
    "\n",
    "    delta_3 = h - y\n",
    "    delta_2 = np.dot(delta_3, Theta2[:, 1:]) * sigmoidGrad(z2)\n",
    "    Theta2_grad = (np.dot(delta_3.T, a2) + (lmbda * np.hstack((np.zeros((Theta2.shape[0], 1)), Theta2[:, 1:])))) / m\n",
    "    Theta1_grad = (np.dot(delta_2.T, a1) + (lmbda * np.hstack((np.zeros((Theta1.shape[0], 1)), Theta1[:, 1:])))) / m\n",
    "\n",
    "    grad = np.hstack((Theta1_grad.flatten(), Theta2_grad.flatten()))\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**nnGrad**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "K = 10\n",
    "lmbda = 0.03\n",
    "epsilon = 0.12\n",
    "\n",
    "input_layer_size = 13\n",
    "hidden_layer_size = 20\n",
    "num_labels = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialisation of relevant parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.genfromtxt('heart.csv', delimiter=',')\n",
    "m, n = X.shape\n",
    "n -= 1\n",
    "\n",
    "y = X[:, n].astype(int).reshape((m, 1))\n",
    "X = featureNormalize(X[:, :n])\n",
    "foldAcc = np.ndarray((K, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the dataset and extract labels and attributes from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FP = 0\n",
    "FN = 0\n",
    "TN = 0\n",
    "TP = 0\n",
    "\n",
    "for i in range(K):\n",
    "    X_train, y_train, X_test, y_test = KFoldDiv(X, y, m, i+1, K)\n",
    "    \n",
    "    initTheta = randomizeTheta((hidden_layer_size * (input_layer_size+1)) + (num_labels * (hidden_layer_size+1)), epsilon)\n",
    "    Theta = optimize.fmin_bfgs(nnCostFunc, initTheta, fprime=nnGrad, args=(input_layer_size, hidden_layer_size, num_labels, X_train, y_train, lmbda), maxiter=3000)\n",
    "    Theta1, Theta2 = np.split(Theta, [hidden_layer_size * (input_layer_size+1)])\n",
    "    Theta1 = np.reshape(Theta1, (hidden_layer_size, input_layer_size+1))\n",
    "    Theta2 = np.reshape(Theta2, (num_labels, hidden_layer_size+1))\n",
    "\n",
    "    h1 = sigmoid(np.dot(np.hstack((np.ones((X_test.shape[0], 1)), X_test)), Theta1.T))\n",
    "    h2 = sigmoid(np.dot(np.hstack((np.ones((h1.shape[0], 1)), h1)), Theta2.T))\n",
    "    predicted = h2.argmax(1) + 1\n",
    "    predicted = predicted.reshape((predicted.shape[0], 1))\n",
    "    foldAcc[i] = np.mean((predicted == y_test).astype(float)) * 100\n",
    "\n",
    "    cm = (metrics.confusion_matrix(y_test, predicted))/len(y_test)\n",
    "\n",
    "    FP += cm[0][0]\n",
    "    FN += cm[1][0]\n",
    "    TN += cm[0][1]\n",
    "    TP += cm[1][1]\n",
    "\n",
    "    print('Test Set Accuracy for %dth fold: %f\\n' % (i+1, foldAcc[i]))\n",
    "\n",
    "meanAcc = np.mean(foldAcc)\n",
    "print('\\nAverage Accuracy: ', meanAcc)\n",
    "print(\"\")\n",
    "\n",
    "print(FP)\n",
    "print(FN)\n",
    "print(TN)\n",
    "print(TP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above written code is used to run 10 Fold Cross Validation on the Neural Network and display the Model Accuracy and the Confusion Matrix and related metrics.\n",
    "\n",
    "**fmin_bfgs** function from **Scipy** library is used to optimize the weights in order to minimize the cost, using the BFGS algorithm.\n",
    "\n",
    "Parameters:\n",
    "- f : callable f(x,\\*args), *Objective function to be minimized.*\n",
    "- x0 : ndarray, *Initial guess.*\n",
    "- fprime : callable f’(x,\\*args), *Gradient of f.*\n",
    "- args : tuple, *Extra arguments passed to f and fprime.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
